{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f885b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F \n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from torchvision import datasets,transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1dbf1e",
   "metadata": {},
   "source": [
    "# training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3df7c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200 #每次的樣本數量\n",
    "learning_rate=0.01 \n",
    "epochs = 10 \n",
    "# 讀取MNIST手寫數字數據集,初次運行下載到../data/目錄下\n",
    "\n",
    "# 要注意所有樣本進行標準化的參數要保持一致(這裡是樣本和方差)\n",
    "# 這個標準化的參數是數據提供方計算好的\n",
    "# 所以就不用自己計算了,在網上查好然後標準化時候寫進去就可以了\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data',train=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), #轉成tensor的類型\n",
    "                       transforms.Normalize((0.1307,),(0.3081,)) \n",
    "                   ])),\n",
    "    batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb0fd7",
   "metadata": {},
   "source": [
    "# testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86af39e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1631d43e888>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data',train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), #轉成tensor的類型\n",
    "                       transforms.Normalize((0.1307,),(0.3081,)) \n",
    "                   ])),\n",
    "    batch_size=batch_size,shuffle=True)\n",
    "test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210670ed",
   "metadata": {},
   "source": [
    "# linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca3e8bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1362, -0.1365, -0.0645,  ..., -0.1168,  0.0714,  0.0454],\n",
       "        [ 0.0589,  0.0033, -0.1465,  ..., -0.0485,  0.0916, -0.1087],\n",
       "        [ 0.0681,  0.1995, -0.0518,  ..., -0.1379,  0.0740,  0.0400],\n",
       "        ...,\n",
       "        [ 0.1376,  0.0130,  0.0884,  ...,  0.0568,  0.2364, -0.2266],\n",
       "        [ 0.0762,  0.1269, -0.0058,  ...,  0.0180,  0.0333, -0.0910],\n",
       "        [-0.0132,  0.0129,  0.1586,  ...,  0.0264,  0.1072,  0.1041]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input為784 out為200\n",
    "w1 = torch.randn(200,784,requires_grad=True)\n",
    "b1 = torch.randn(200,requires_grad=True)\n",
    "w2 = torch.randn(200,200,requires_grad=True)\n",
    "b2 = torch.randn(200,requires_grad=True)\n",
    "w3 = torch.randn(10,200,requires_grad=True)\n",
    "b3 = torch.randn(10,requires_grad=True)\n",
    "#下面這三行很重要 若沒初始化可能會梯度消失\n",
    "torch.nn.init.kaiming_normal_(w1)\n",
    "torch.nn.init.kaiming_normal_(w2)\n",
    "torch.nn.init.kaiming_normal_(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb44c3",
   "metadata": {},
   "source": [
    "# linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf186000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x): #x 的matrix為[樣本數,784]\n",
    "    x=x@w1.t()+b1\n",
    "    x=F.relu(x)\n",
    "    x=x@w2.t()+b2 \n",
    "    x=F.relu(x)\n",
    "    x=x@w3.t()+b3 \n",
    "    x=F.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fd278",
   "metadata": {},
   "source": [
    "#  選擇LOSS 跟GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad4bf966",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD([w1,b1,w2,b2,w3,b3],lr=1e-3)\n",
    "CEL = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2bcd0",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632291a",
   "metadata": {},
   "source": [
    "只有在資料很龐大的時候（在機器學習中，資料一般情況下都會很大），我們才需要使用epochs，batch size，iteration這些術語，在這種情況下，一次性將資料輸入計算機是不可能的。因此，為了解決這個問題，我們需要把資料分成小塊，一塊一塊的傳遞給計算機，在每一步的末端更新神經網路的權重，擬合給定的資料。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9063daa",
   "metadata": {},
   "source": [
    "# epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4830164",
   "metadata": {},
   "source": [
    "當一個完整的資料集通過了神經網路一次並且返回了一次，這個過程稱為一次epoch。然而，當一個epoch對於計算機而言太龐大的時候，就需要把它分成多個小塊。\n",
    "在神經網路中傳遞完整的資料集一次是不夠的，而且我們需要將完整的資料集在同樣的神經網路中傳遞多次。但請記住，我們使用的是有限的資料集，並且我們使用一個迭代過程即梯度下降來優化學習過程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f70553",
   "metadata": {},
   "source": [
    "### 隨著epoch數量增加，神經網路中的權重的更新次數也在增加，曲線從欠擬合變得過擬合。對於不同的資料集，epoch的數量答案是不一樣的。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af5acfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0[0/60000 (0%)]\tLoss:0.649469\n",
      "Train Epoch: 0[20000/60000 (33%)]\tLoss:0.639419\n",
      "Train Epoch: 0[40000/60000 (67%)]\tLoss:0.569756\n",
      "\n",
      "Tset set:Average loss:0.0029,Accuracy:8177/10000(82%)\n",
      "\n",
      "Train Epoch: 1[0/60000 (0%)]\tLoss:0.564435\n",
      "Train Epoch: 1[20000/60000 (33%)]\tLoss:0.583145\n",
      "Train Epoch: 1[40000/60000 (67%)]\tLoss:0.392156\n",
      "\n",
      "Tset set:Average loss:0.0028,Accuracy:8232/10000(82%)\n",
      "\n",
      "Train Epoch: 2[0/60000 (0%)]\tLoss:0.485450\n",
      "Train Epoch: 2[20000/60000 (33%)]\tLoss:0.540783\n",
      "Train Epoch: 2[40000/60000 (67%)]\tLoss:0.457254\n",
      "\n",
      "Tset set:Average loss:0.0027,Accuracy:8257/10000(83%)\n",
      "\n",
      "Train Epoch: 3[0/60000 (0%)]\tLoss:0.695377\n",
      "Train Epoch: 3[20000/60000 (33%)]\tLoss:0.497880\n",
      "Train Epoch: 3[40000/60000 (67%)]\tLoss:0.575176\n",
      "\n",
      "Tset set:Average loss:0.0027,Accuracy:8271/10000(83%)\n",
      "\n",
      "Train Epoch: 4[0/60000 (0%)]\tLoss:0.454336\n",
      "Train Epoch: 4[20000/60000 (33%)]\tLoss:0.545366\n",
      "Train Epoch: 4[40000/60000 (67%)]\tLoss:0.489399\n",
      "\n",
      "Tset set:Average loss:0.0026,Accuracy:8285/10000(83%)\n",
      "\n",
      "Train Epoch: 5[0/60000 (0%)]\tLoss:0.578987\n",
      "Train Epoch: 5[20000/60000 (33%)]\tLoss:0.559829\n",
      "Train Epoch: 5[40000/60000 (67%)]\tLoss:0.522703\n",
      "\n",
      "Tset set:Average loss:0.0026,Accuracy:8306/10000(83%)\n",
      "\n",
      "Train Epoch: 6[0/60000 (0%)]\tLoss:0.615311\n",
      "Train Epoch: 6[20000/60000 (33%)]\tLoss:0.485323\n",
      "Train Epoch: 6[40000/60000 (67%)]\tLoss:0.597380\n",
      "\n",
      "Tset set:Average loss:0.0026,Accuracy:8313/10000(83%)\n",
      "\n",
      "Train Epoch: 7[0/60000 (0%)]\tLoss:0.552089\n",
      "Train Epoch: 7[20000/60000 (33%)]\tLoss:0.505439\n",
      "Train Epoch: 7[40000/60000 (67%)]\tLoss:0.539501\n",
      "\n",
      "Tset set:Average loss:0.0025,Accuracy:8326/10000(83%)\n",
      "\n",
      "Train Epoch: 8[0/60000 (0%)]\tLoss:0.542007\n",
      "Train Epoch: 8[20000/60000 (33%)]\tLoss:0.621892\n",
      "Train Epoch: 8[40000/60000 (67%)]\tLoss:0.646780\n",
      "\n",
      "Tset set:Average loss:0.0025,Accuracy:8339/10000(83%)\n",
      "\n",
      "Train Epoch: 9[0/60000 (0%)]\tLoss:0.529759\n",
      "Train Epoch: 9[20000/60000 (33%)]\tLoss:0.516378\n",
      "Train Epoch: 9[40000/60000 (67%)]\tLoss:0.523797\n",
      "\n",
      "Tset set:Average loss:0.0025,Accuracy:8348/10000(83%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "#training\n",
    "    for batch_index,(data,target) in enumerate(train_loader):\n",
    "        #將data攤平成[樣本數,784]\n",
    "        data=data.reshape(-1,28*28)\n",
    "        logits=forward(data) #計算logits\n",
    "        loss=CEL(logits,target) #計算Loss，且不需要softmax因為pytorch的cel已經有了\n",
    "        optimizer.zero_grad() #清空梯度\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index%100 ==0:\n",
    "            print('Train Epoch: {}[{}/{} ({:.0f}%)]\\tLoss:{:.6f}'.format(\n",
    "            epoch,batch_index*len(data),len(train_loader.dataset),\n",
    "            100.*batch_index/len(train_loader),loss.item()))\n",
    "#   testing  \n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    for data , target in test_loader:\n",
    "        data=data.reshape(-1,28*28)\n",
    "        logits=forward(data)\n",
    "        test_loss+=CEL(logits,target).item()\n",
    "        #得到的預測值0~9 10個vector的機率，在第二個為杜取max\n",
    "        #logits.data是一個shape=[batch_size,10]的Tensor\n",
    "        #第一個tensor存的是最大值的值，第二個tensor是對應的索引\n",
    "        pred=logits.data.max(dim=1)[1]\n",
    "        correct+=pred.eq(target.data).sum()\n",
    "    test_loss/=len(test_loader.dataset)\n",
    "    print('\\nTset set:Average loss:{:.4f},Accuracy:{}/{}({:.0f}%)\\n'.format(\n",
    "        test_loss,correct,len(test_loader.dataset),\n",
    "        100.*correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6ce6b",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80b2c53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tset set:Average loss:0.0043,Accuracy:7217/10000(72%)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b497cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
